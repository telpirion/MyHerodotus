# Week 4 Development Log

+ Unhappy: ğŸ‘
+ Anxious: ğŸ˜¬
+ Curious: ğŸ¤”
+ Happy: ğŸ‘

## Objectives

For this week's activities, we must do the following:

- [ ] Set up an evaluation pipeline to compare Gemini, Gemma, and/or tuned model.
- [ ] Export evaluation to a "table" (BQ?).
- [ ] Set up a rapid evaluation pipeline to see the specific performance of a model.

Nice to haves:

- [ ] Limit context passed to Gemma model based upon token count
- [ ] Train Gemma model on Guanaco dataset

## Set up an evaluation pipeline

+ ğŸ˜¬ğŸ¤” The evaluation overview in the docs reads more like marketing copy than a 
  technical overview.
+ In the quickstart, are "Fluency" and "Entertaining" both metrics that are pre-defined?
  Or can these metrics be any arbitrary measurement provided that they have a explanation?

Sources:

+ https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview